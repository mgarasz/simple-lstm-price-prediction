{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import deque\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM, CuDNNLSTM, BatchNormalization\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.callbacks import ModelCheckpoint, ModelCheckpoint\n",
    "\n",
    "import time\n",
    "import ta\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from custom_utils import classify, moving_average, moving_average_convergence, Bolinger_Bands, preprocess_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''GENERAL PARAMS'''\n",
    "SEQ_LEN = 30  # how long of a preceeding sequence to collect for RNN\n",
    "FUTURE_PERIOD_PREDICT = 3  # how far into the future are we trying to predict?\n",
    "RATIO_TO_PREDICT = \"LTC-USD\"\n",
    "SMA_WINDOW = 7\n",
    "MACD_LOW = 26\n",
    "MACD_FAST = 12\n",
    "THRESHOLD = 1.001 #You can use this to encode trading fee. I.e., 0.1% per trade.\n",
    "\n",
    "'''NEURAL NET PARAMS'''\n",
    "EPOCHS = 12 # how many passes through our data\n",
    "BATCH_SIZE = 64  # how many batches? Try smaller batch if you're getting OOM (out of memory) errors.\n",
    "NAME = f\"{SEQ_LEN}-SEQ-{FUTURE_PERIOD_PREDICT}-PRED-{int(time.time())}\"\n",
    "\n",
    "'''DATA PARMS'''\n",
    "unix_end = round(time.time()) \n",
    "unix_start = unix_end-86400*530\n",
    "period = 300 #seconds\n",
    "pair = 'USDT_LTC'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Bitcoin data from Poloniex API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = f'https://poloniex.com/public?command=returnChartData&currencyPair={pair}&start={unix_start}&end={unix_end}&period={period}'\n",
    "data = pd.read_json(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Technical indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['date'] = data['date'].astype(np.int64) // 10**9\n",
    "data.set_index('date', inplace=True)\n",
    "\n",
    "data = data[['close','volume']]\n",
    "data.rename(columns={\"close\": \"LTC-USD_close\", \"volume\": \"LTC-USD_volume\"}, inplace=True)\n",
    "\n",
    "data[\"LTC-USD_SMA\"] = moving_average(data[\"LTC-USD_close\"])\n",
    "data[\"LTC-USD_MACD\"] = moving_average_convergence(data[\"LTC-USD_close\"])['MACD']\n",
    "data[\"LTC-USD_VPT\"] = ta.volume_price_trend(data[\"LTC-USD_close\"], df[\"LTC-USD_volume\"])\n",
    "\n",
    "data.fillna(method=\"ffill\", inplace=True)  # if there are gaps in data, use previously known values\n",
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Y column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['future'] = data['LTC-USD_close'].shift(-FUTURE_PERIOD_PREDICT)\n",
    "data['target'] = list(map(classify, main_df2['LTC-USD_close'], data['future']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## here, split away some slice of the future data from the main main_df.\n",
    "times = sorted(data.index.values)\n",
    "last_5pct = sorted(data.index.values)[-int(0.05*len(times))]\n",
    "\n",
    "validation_data = data[(data.index >= last_5pct)]\n",
    "data = data[(data.index < last_5pct)]\n",
    "\n",
    "train_x, train_y = preprocess_df(data)\n",
    "validation_x, validation_y = preprocess_df(validation_data)\n",
    "\n",
    "print(f\"train data: {len(train_x)} validation: {len(validation_x)}\")\n",
    "print(f\"Hold: {train_y.count(0)}, buys: {train_y.count(1)}, sell: {train_y.count(2)}\")\n",
    "print(f\"VALIDATION Hold: {validation_y.count(0)}, buys: {validation_y.count(1)}, sell: {validation_y.count(2)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(CuDNNLSTM(128, input_shape=(train_x.shape[1:]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(CuDNNLSTM(128, return_sequences=True))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(CuDNNLSTM(128))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(3, activation='softmax'))   #made a change here! originally it was 2\n",
    "\n",
    "opt = keras.optimizers.Adam(lr=0.001, decay=1e-6)\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=opt,\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(NAME))\n",
    "\n",
    "filepath = \"RNN_Final-{epoch:02d}-{val_acc:.3f}\"  # unique file name that will include the epoch and the validation acc for that epoch\n",
    "checkpoint = ModelCheckpoint(\"models/{}.model\".format(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')) # saves only the best ones\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "history = model.fit(\n",
    "    train_x, train_y,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(validation_x, validation_y),\n",
    "    callbacks=[tensorboard, checkpoint],\n",
    ")\n",
    "\n",
    "# Score model\n",
    "score = model.evaluate(validation_x, validation_y, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "# Save model\n",
    "model.save(\"output/models/{}\".format(NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  \"Accuracy\"\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "# \"Loss\"\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
